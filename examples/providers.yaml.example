# FreeLLM Providers Configuration Example
# Copy this to providers.yaml and configure

providers:
  # OpenRouter - Fetches all available models via API
  - type: openrouter
    enabled: true
    api_key: ${OPENROUTER_API_KEY}

  # Ollama - Auto-discovers locally installed models
  - type: ollama
    enabled: false
    api_base: http://localhost:11434

  # ModelScope/DashScope - Chinese LLM platform
  - type: modelscope
    enabled: false
    api_key: ${MODELSCOPE_API_KEY}
    models:
      - qwen-turbo
      - qwen-plus

  # iFlow - Free Chinese AI models (all free)
  - type: iflow
    enabled: false
    api_key: ${IFLOW_API_KEY}

  # Static - For any custom OpenAI-compatible endpoint
  - type: static
    enabled: false
    model_name: gpt-3.5-turbo
    provider: openai
    api_base: https://api.example.com/v1
    api_key: ${YOUR_API_KEY}

  # Multiple static providers example
  - type: static
    enabled: false
    model_name: claude-3-sonnet
    provider: anthropic
    api_base: https://api.example.com/v1
    api_key: ${ANOTHER_KEY}
